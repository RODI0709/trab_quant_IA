{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2702475f",
   "metadata": {},
   "source": [
    "# **1. Filtrando Ações e df ex_R_acoes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38e85d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Período: 2013-12-31 → 2024-12-31 | n_meses = 133\n",
      "Ações com série completa (sem NA) em toda a amostra: 114\n",
      "Exemplos de tickers: ABCB4, ABEV3, AGRO3, ALPA4, ALUP11, AMER3, ANIM3, AZZA3, B3SA3, BBAS3 ... (+104 mais)\n"
     ]
    }
   ],
   "source": [
    "# === Carregar e filtrar ações com log-returns mensais completos (sem NA) ===\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Caminho do arquivo\n",
    "xlsx_path = Path(\"Base_Final_DQAI2025_v3.xlsx\")\n",
    "\n",
    "# 1) Ler a sheet \"Acoes Log Ret M\"\n",
    "#    - Converte a coluna \"Data\" para datetime (formato dd/mm/yyyy hh:mm:ss)\n",
    "#    - Mantém a ordem mensal EOM\n",
    "df_raw = pd.read_excel(\n",
    "    xlsx_path,\n",
    "    sheet_name=\"Acoes Log Ret M\"\n",
    ")\n",
    "\n",
    "# Garantir que a coluna 'Data' exista\n",
    "assert \"Data\" in df_raw.columns, \"A sheet 'Acoes Log Ret M' deve conter a coluna 'Data'.\"\n",
    "\n",
    "# 2) Tratar a coluna de datas\n",
    "df_raw[\"Data\"] = pd.to_datetime(df_raw[\"Data\"], dayfirst=True, errors=\"coerce\")\n",
    "# Remover linhas com Data inválida (se houver)\n",
    "df_raw = df_raw.dropna(subset=[\"Data\"]).sort_values(\"Data\").reset_index(drop=True)\n",
    "\n",
    "# 3) Separar colunas\n",
    "date_col = [\"Data\"]\n",
    "action_cols = [c for c in df_raw.columns if c != \"Data\"]\n",
    "\n",
    "# (Opcional, robustez) Assegurar que as colunas de ações sejam numéricas\n",
    "# Caso haja strings acidentais, isso converte para numérico e introduz NaN onde não for número\n",
    "df_actions = df_raw[action_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "# 4) Remover colunas com qualquer NA ao longo da amostra\n",
    "mask_full = ~df_actions.isna().any(axis=0)\n",
    "full_cols = [col for col in df_actions.columns if mask_full[col]]\n",
    "\n",
    "# 5) Montar o DataFrame final\n",
    "df_clean = pd.concat([df_raw[date_col], df_actions[full_cols]], axis=1)\n",
    "\n",
    "# 6) Quantidade de ações remanescentes\n",
    "n_actions = len(full_cols)\n",
    "\n",
    "# 7) Prints de checagem\n",
    "print(f\"Período: {df_clean['Data'].min().date()} → {df_clean['Data'].max().date()} | n_meses = {df_clean.shape[0]}\")\n",
    "print(f\"Ações com série completa (sem NA) em toda a amostra: {n_actions}\")\n",
    "if n_actions > 0:\n",
    "    preview = \", \".join(full_cols[:10])\n",
    "    extra = \"\" if n_actions <= 10 else f\" ... (+{n_actions-10} mais)\"\n",
    "    print(f\"Exemplos de tickers: {preview}{extra}\")\n",
    "else:\n",
    "    print(\"Nenhuma ação possui série completa. Verifique a consistência da base ou relaxe o critério.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93129b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ex_R_acoes: 133 meses × 114 ações (excessos)\n",
      "Período alinhado: 2013-12-31 → 2024-12-31\n",
      "Coluna de CDI utilizada: CDI_LogRet_M\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Pré-requisito: df_clean já existe em memória ---\n",
    "# df_clean: Data + 114 ações sem NA (criado no passo anterior)\n",
    "\n",
    "xlsx_path = Path(\"Base_Final_DQAI2025_v3.xlsx\")\n",
    "\n",
    "# 1) Ler a sheet de CDI (log return mensal)\n",
    "df_cdi = pd.read_excel(xlsx_path, sheet_name=\"CDI Log Ret M\")\n",
    "\n",
    "# 2) Tratar datas e identificar a coluna do CDI (primeira não-'Data')\n",
    "assert \"Data\" in df_cdi.columns, \"A sheet 'CDI Log Ret M' deve conter a coluna 'Data'.\"\n",
    "df_cdi[\"Data\"] = pd.to_datetime(df_cdi[\"Data\"], dayfirst=True, errors=\"coerce\")\n",
    "df_cdi = df_cdi.dropna(subset=[\"Data\"]).sort_values(\"Data\").reset_index(drop=True)\n",
    "\n",
    "cdi_cols = [c for c in df_cdi.columns if c != \"Data\"]\n",
    "assert len(cdi_cols) >= 1, \"A sheet 'CDI Log Ret M' precisa ter ao menos 1 coluna de CDI além de 'Data'.\"\n",
    "cdi_col = cdi_cols[0]  # usa a primeira coluna de CDI disponível\n",
    "\n",
    "# Garantir numérico\n",
    "df_cdi[cdi_col] = pd.to_numeric(df_cdi[cdi_col], errors=\"coerce\")\n",
    "\n",
    "# 3) Alinhar datas com df_clean (inner join)\n",
    "df_merged = df_clean.merge(df_cdi[[\"Data\", cdi_col]], on=\"Data\", how=\"inner\")\n",
    "\n",
    "# 4) Calcular excesso de retorno log para cada ação\n",
    "action_cols = [c for c in df_clean.columns if c != \"Data\"]\n",
    "cdi_series = df_merged[cdi_col]\n",
    "\n",
    "excess_values = df_merged[action_cols].sub(cdi_series, axis=0)\n",
    "\n",
    "# 5) Montar o DataFrame final: Data + 114 colunas de excessos\n",
    "ex_R_acoes = pd.concat([df_merged[[\"Data\"]], excess_values], axis=1)\n",
    "\n",
    "# 6) Checagens\n",
    "print(f\"ex_R_acoes: {ex_R_acoes.shape[0]} meses × {ex_R_acoes.shape[1]-1} ações (excessos)\")\n",
    "print(f\"Período alinhado: {ex_R_acoes['Data'].min().date()} → {ex_R_acoes['Data'].max().date()}\")\n",
    "print(f\"Coluna de CDI utilizada: {cdi_col}\")\n",
    "\n",
    "# (Opcional) Salvar em disco\n",
    "ex_R_acoes.to_parquet(\"ex_R_acoes.parquet\", index=False)\n",
    "ex_R_acoes.to_csv(\"ex_R_acoes.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef90e668",
   "metadata": {},
   "source": [
    "# **2. Random Fourier Features (RFFs)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "debf8356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meses na base original de fatores: 133\n",
      "Meses após padronização expansiva (mínimo 24 meses): 110\n",
      "Período em G_t: 2015-11-30 → 2024-12-31\n",
      "Número de fatores em G_t: 148\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Caminho do arquivo\n",
    "xlsx_path = Path(\"Base_Final_DQAI2025_v3.xlsx\")\n",
    "\n",
    "# 1) Ler a sheet de fatores mensais (Bryan Kelly Brasil)\n",
    "#    OBS: o nome da aba é \"Fatores BK M\" (ajuste se estiver diferente no seu arquivo)\n",
    "df_fatores = pd.read_excel(\n",
    "    xlsx_path,\n",
    "    sheet_name=\"Fatores BK M\"\n",
    ")\n",
    "\n",
    "# 2) Tratar coluna de datas\n",
    "assert \"Data\" in df_fatores.columns, \"A sheet 'Fatoes BK M' deve conter a coluna 'Data'.\"\n",
    "\n",
    "df_fatores[\"Data\"] = pd.to_datetime(df_fatores[\"Data\"], dayfirst=True, errors=\"coerce\")\n",
    "df_fatores = (\n",
    "    df_fatores\n",
    "    .dropna(subset=[\"Data\"])\n",
    "    .sort_values(\"Data\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# 3) Separar colunas de fatores (148 colunas além de 'Data')\n",
    "date_col = \"Data\"\n",
    "factor_cols = [c for c in df_fatores.columns if c != date_col]\n",
    "\n",
    "# Garantir que todos os fatores estão em formato numérico\n",
    "df_X = df_fatores[factor_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "# 4) Calcular desvio padrão expansivo com mínimo de 24 meses\n",
    "#    expanding().std() usa ddof=1 (desvio padrão amostral), o que é ok para escala\n",
    "rolling_std = df_X.expanding(min_periods=24).std()\n",
    "\n",
    "# 5) Padronizar: fator / desvio padrão expansivo\n",
    "df_X_std = df_X / rolling_std\n",
    "\n",
    "# 6) Montar o DataFrame final G_t:\n",
    "#    - Mantemos a coluna 'Data'\n",
    "#    - Removemos as linhas iniciais onde ainda não havia 24 meses (std = NaN)\n",
    "df_Gt = pd.concat([df_fatores[[date_col]], df_X_std], axis=1)\n",
    "\n",
    "# Eliminar linhas com qualquer NaN nos fatores (período antes dos 24 meses)\n",
    "mask_valid = ~df_Gt[factor_cols].isna().any(axis=1)\n",
    "df_Gt = df_Gt.loc[mask_valid].reset_index(drop=True)\n",
    "\n",
    "# 7) Checagens rápidas\n",
    "n_meses_original = df_fatores.shape[0]\n",
    "n_meses_Gt = df_Gt.shape[0]\n",
    "\n",
    "print(f\"Meses na base original de fatores: {n_meses_original}\")\n",
    "print(f\"Meses após padronização expansiva (mínimo 24 meses): {n_meses_Gt}\")\n",
    "print(f\"Período em G_t: {df_Gt['Data'].min().date()} → {df_Gt['Data'].max().date()}\")\n",
    "print(f\"Número de fatores em G_t: {len(factor_cols)}\")\n",
    "\n",
    "# (Opcional) salvar em disco para reutilizar depois\n",
    "df_Gt.to_parquet(\"Gt_fatoresBK_expanding24.parquet\", index=False)\n",
    "df_Gt.to_csv(\"Gt_fatoresBK_expanding24.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "6b4b313c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nº de meses em G_t: 110, Nº de fatores: 148\n",
      "Shape de G: (110, 148) (T x d)\n",
      "Shape de S_full (RFF): (110, 12000) (T x P)\n",
      "        Data     RFF_1     RFF_2     RFF_3     RFF_4     RFF_5     RFF_6  \\\n",
      "0 2015-11-30  0.864096  0.674262 -0.686221  0.950084 -0.998861 -0.372674   \n",
      "1 2015-12-31 -0.805667  0.500175 -0.645283 -0.518692 -0.945397 -0.984444   \n",
      "2 2016-01-31  0.442689  0.813684  0.211457 -0.909946  0.361474  0.996318   \n",
      "3 2016-02-29 -0.652902 -0.089506 -0.109048  0.832404  0.504901  0.098547   \n",
      "4 2016-03-31 -0.378944  0.857365 -0.244482  0.262824  0.954738 -0.696470   \n",
      "\n",
      "      RFF_7     RFF_8     RFF_9  ...  RFF_11991  RFF_11992  RFF_11993  \\\n",
      "0  0.974051 -0.193734 -0.192409  ...  -0.798717  -0.947031   0.945496   \n",
      "1 -0.377093 -0.490091 -0.989208  ...  -0.822754  -0.279857  -0.334332   \n",
      "2  0.829912 -0.900614 -0.997620  ...  -0.538275  -0.926270   0.999826   \n",
      "3 -0.806354 -0.768842 -0.577185  ...   0.970832  -0.461626  -0.736420   \n",
      "4  0.981423 -0.971630 -0.142928  ...   0.111635  -0.629271   0.971102   \n",
      "\n",
      "   RFF_11994  RFF_11995  RFF_11996  RFF_11997  RFF_11998  RFF_11999  RFF_12000  \n",
      "0  -0.928167  -0.417155  -0.621801  -0.966161   0.937436   0.753327   0.225673  \n",
      "1  -0.150598   0.969856  -0.881169   0.815041   0.021826  -0.240230  -0.997363  \n",
      "2  -0.311103   0.312844  -0.966544   0.565401  -0.104750   0.354580  -0.022418  \n",
      "3  -0.674149  -0.411745  -0.402533   0.218906  -0.673352  -0.362461  -0.978230  \n",
      "4   0.106983  -0.817804   0.969066  -0.330799  -0.999268   0.965191   0.965474  \n",
      "\n",
      "[5 rows x 12001 columns]\n",
      "df_St: 110 meses × 12000 RFFs\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# =========================================================\n",
    "# 1) Carregar G_t padronizado (se df_Gt já estiver em memória, pode pular o read)\n",
    "# =========================================================\n",
    "# Se você já tem df_Gt carregado, comente as duas linhas abaixo\n",
    "gt_path = Path(\"Gt_fatoresBK_expanding24.parquet\")  # ajuste se usou outro nome\n",
    "df_Gt = pd.read_parquet(gt_path)\n",
    "\n",
    "# Checar estrutura\n",
    "assert \"Data\" in df_Gt.columns, \"df_Gt deve conter a coluna 'Data'.\"\n",
    "factor_cols = [c for c in df_Gt.columns if c != \"Data\"]\n",
    "print(f\"Nº de meses em G_t: {df_Gt.shape[0]}, Nº de fatores: {len(factor_cols)}\")\n",
    "\n",
    "# =========================================================\n",
    "# 2) Preparar matriz de fatores G (T x d)\n",
    "# =========================================================\n",
    "G = df_Gt[factor_cols].to_numpy(dtype=float)   # shape (T, 148)\n",
    "T, d = G.shape\n",
    "print(f\"Shape de G: {G.shape} (T x d)\")\n",
    "\n",
    "# =========================================================\n",
    "# 3) Gerar Random Fourier Features\n",
    "#    - P = 12.000 (6.000 senos + 6.000 cossenos)\n",
    "#    - γ = 2\n",
    "#    - seed fixa = 123\n",
    "# =========================================================\n",
    "P = 12000\n",
    "half_P = P // 2\n",
    "gamma = 1.0\n",
    "\n",
    "rng = np.random.default_rng(123)\n",
    "\n",
    "# Ω ~ N(0, I_d), com shape (d, 6000)\n",
    "Omega = rng.normal(loc=0.0, scale=1.0, size=(d, half_P))\n",
    "\n",
    "# Projeções lineares: A = G @ Ω  → shape (T, 6000)\n",
    "A = G @ Omega\n",
    "\n",
    "# RFF:\n",
    "# Primeiro 6000 colunas: sin(γ * A)\n",
    "# Próximas 6000:        cos(γ * A)\n",
    "S_sin = np.sin(gamma * A)         # (T, 6000)\n",
    "S_cos = np.cos(gamma * A)         # (T, 6000)\n",
    "\n",
    "# Concatenar para formar 12.000 features\n",
    "S_full = np.concatenate([S_sin, S_cos], axis=1)   # shape (T, 12000)\n",
    "\n",
    "print(f\"Shape de S_full (RFF): {S_full.shape} (T x P)\")\n",
    "\n",
    "# =========================================================\n",
    "# 4) Montar df_St: Data + 12.000 RFF\n",
    "# =========================================================\n",
    "rff_cols = [f\"RFF_{k+1}\" for k in range(P)]  # RFF_1, ..., RFF_12000\n",
    "\n",
    "df_St = pd.DataFrame(S_full, columns=rff_cols)\n",
    "df_St.insert(0, \"Data\", df_Gt[\"Data\"].values)\n",
    "\n",
    "# Checagens\n",
    "print(df_St.head())\n",
    "print(f\"df_St: {df_St.shape[0]} meses × {df_St.shape[1]-1} RFFs\")\n",
    "\n",
    "# (Opcional) salvar para reutilizar depois\n",
    "df_St.to_parquet(\"St_RFF_12000_g2_seed123.parquet\", index=False)\n",
    "df_St.to_csv(\"St_RFF_12000_g2_seed123.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a468d890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_St carregado: 110 meses × 6000 RFFs\n",
      "Heatmap salvo em: Gráficos de Preparação\\heatmapRFF.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from pathlib import Path\n",
    "\n",
    "# ================================\n",
    "# 1) Carregar df_St (ajuste o nome se precisar)\n",
    "# ================================\n",
    "base_dir = Path(\".\")  # diretório atual do seu projeto\n",
    "\n",
    "# Ajuste aqui se você usou outro nome\n",
    "parquet_path = base_dir / \"St_RFF_12000_g2_seed123.parquet\"\n",
    "csv_path = base_dir / \"St_RFF_12000_g2_seed123.csv\"\n",
    "\n",
    "if parquet_path.exists():\n",
    "    df_St = pd.read_parquet(parquet_path)\n",
    "elif csv_path.exists():\n",
    "    df_St = pd.read_csv(csv_path, parse_dates=[\"Data\"])\n",
    "else:\n",
    "    raise FileNotFoundError(\"Nem parquet nem csv de df_St foram encontrados no diretório atual.\")\n",
    "\n",
    "print(f\"df_St carregado: {df_St.shape[0]} meses × {df_St.shape[1]-1} RFFs\")\n",
    "\n",
    "# ================================\n",
    "# 2) Selecionar 100 primeiras RFFs\n",
    "# ================================\n",
    "rff_cols = [c for c in df_St.columns if c.startswith(\"RFF_\")]\n",
    "rff_cols_100 = rff_cols[:100]\n",
    "\n",
    "df_subset = df_St[rff_cols_100]\n",
    "\n",
    "# ================================\n",
    "# 3) Matriz de correlação (|corr|)\n",
    "# ================================\n",
    "corr_matrix = df_subset.corr().abs()\n",
    "\n",
    "# ================================\n",
    "# 4) Colormap roxo: #F0DFFF → #220042\n",
    "# ================================\n",
    "colors = [\"#B98DE2\", \"#220042\"]\n",
    "purple_cmap = LinearSegmentedColormap.from_list(\"custom_purple\", colors)\n",
    "\n",
    "# ================================\n",
    "# 5) Plot do heatmap\n",
    "# ================================\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "im = ax.imshow(corr_matrix, cmap=purple_cmap, vmin=0, vmax=1)\n",
    "\n",
    "num_features = len(rff_cols_100)\n",
    "tick_positions = np.arange(0, num_features, 10)\n",
    "tick_labels = [rff_cols_100[i] for i in tick_positions]\n",
    "\n",
    "ax.set_xticks(tick_positions)\n",
    "ax.set_yticks(tick_positions)\n",
    "ax.set_xticklabels(tick_labels, rotation=90)\n",
    "ax.set_yticklabels(tick_labels)\n",
    "\n",
    "ax.set_title(\"Heatmap de Correlação |RFF| (100 primeiras features)\")\n",
    "cbar = fig.colorbar(im, ax=ax)\n",
    "cbar.set_label(\"|Correlação|\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# ================================\n",
    "# 6) Criar pasta e salvar imagem\n",
    "# ================================\n",
    "output_dir = base_dir / \"Gráficos de Preparação\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "output_path = output_dir / \"heatmapRFF.png\"\n",
    "fig.savefig(output_path, dpi=300, bbox_inches=\"tight\")\n",
    "plt.close(fig)\n",
    "\n",
    "print(f\"Heatmap salvo em: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b1fc47",
   "metadata": {},
   "source": [
    "# **3. Excesso de Retornos $t + 1$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "4fee49b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Data     ABCB4     ABEV3     AGRO3     ALPA4    ALUP11     AMER3  \\\n",
      "0 2015-11-30 -0.058535 -0.046484  0.033895 -0.066233 -0.086251  0.019800   \n",
      "1 2015-12-31 -0.028165  0.033885 -0.159821  0.013102 -0.070833 -0.149009   \n",
      "2 2016-01-31  0.079115 -0.059187  0.150183  0.188459  0.008443 -0.148032   \n",
      "3 2016-02-29  0.323309  0.053213  0.001825 -0.142786  0.151971  0.202036   \n",
      "4 2016-03-31 -0.047761  0.019348  0.049614  0.166441 -0.004281 -0.053353   \n",
      "\n",
      "      ANIM3     AZZA3     B3SA3  ...     TUPY3     UCAS3     UGPA3     UNIP6  \\\n",
      "0  0.083402 -0.049266 -0.033412  ... -0.078255  0.099524 -0.048569  0.103354   \n",
      "1 -0.334579 -0.110577 -0.070086  ... -0.062069 -0.039995 -0.028353 -0.055366   \n",
      "2  0.157551 -0.074644  0.114510  ... -0.008277  0.036827  0.059749  0.150378   \n",
      "3 -0.182065  0.259298  0.268802  ... -0.086815  0.170783  0.093201  0.218985   \n",
      "4  0.060466  0.027330  0.100189  ... -0.200110  0.040566  0.026502  0.029548   \n",
      "\n",
      "      USIM3     USIM5     VALE3     VLID3     WEGE3     YDUQ3  \n",
      "0 -0.495139 -0.361749 -0.022233 -0.018037 -0.024303  0.027933  \n",
      "1 -0.241755 -0.611268 -0.303563 -0.074123  0.023051 -0.185530  \n",
      "2  0.071283  0.047193  0.184796 -0.025379 -0.163148  0.075907  \n",
      "3  0.163632  0.704066  0.237516 -0.279469  0.052517 -0.088061  \n",
      "4  0.085043  0.312474  0.251621  0.142329  0.069718  0.026014  \n",
      "\n",
      "[5 rows x 115 columns]\n",
      "          Data     ABCB4     ABEV3     AGRO3     ALPA4    ALUP11     AMER3  \\\n",
      "105 2024-08-31 -0.073171  0.008646 -0.035705 -0.110796 -0.049361 -0.224122   \n",
      "106 2024-09-30 -0.049982 -0.043455 -0.012690  0.021927 -0.049650 -0.237771   \n",
      "107 2024-10-31 -0.041438 -0.000804 -0.005354 -0.137255 -0.059580  0.796403   \n",
      "108 2024-11-30 -0.013486 -0.036579 -0.074459  0.000208 -0.054052 -0.297356   \n",
      "109 2024-12-31       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "\n",
      "        ANIM3     AZZA3     B3SA3  ...     TUPY3     UCAS3     UGPA3  \\\n",
      "105 -0.341745 -0.158428 -0.168625  ... -0.070519 -0.061427 -0.104869   \n",
      "106  0.072441 -0.031537 -0.017676  ... -0.105520 -0.027586 -0.024439   \n",
      "107 -0.183186 -0.124803 -0.144934  ... -0.129329 -0.045639 -0.160136   \n",
      "108 -0.312652 -0.207611  0.099109  ...  0.135440 -0.013633 -0.131244   \n",
      "109       NaN       NaN       NaN  ...       NaN       NaN       NaN   \n",
      "\n",
      "        UNIP6     USIM3     USIM5     VALE3     VLID3     WEGE3     YDUQ3  \n",
      "105 -0.022160 -0.021540 -0.006713  0.055561 -0.010517 -0.002643 -0.077672  \n",
      "106 -0.051507  0.048938  0.048367 -0.032332  0.045201 -0.014398  0.134444  \n",
      "107  0.195865 -0.049571 -0.081658 -0.062199  0.038824 -0.011045 -0.145072  \n",
      "108 -0.093908 -0.147725 -0.152623 -0.075069 -0.013108 -0.029763 -0.087963  \n",
      "109       NaN       NaN       NaN       NaN       NaN       NaN       NaN  \n",
      "\n",
      "[5 rows x 115 columns]\n",
      "Shape ex_R_acoes_tmais1_treino: 110 meses × 114 ações\n",
      "Última linha com NaN em todas as ações (esperado): True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 1) Carregar df_St (apenas para pegar as datas)\n",
    "# ---------------------------------------------\n",
    "st_path_parquet = Path(\"St_RFF_12000_g2_seed123.parquet\")\n",
    "st_path_csv     = Path(\"St_RFF_12000_g2_seed123.csv\")\n",
    "\n",
    "if st_path_parquet.exists():\n",
    "    df_St = pd.read_parquet(st_path_parquet)\n",
    "elif st_path_csv.exists():\n",
    "    df_St = pd.read_csv(st_path_csv, parse_dates=[\"Data\"])\n",
    "else:\n",
    "    raise FileNotFoundError(\"Não encontrei df_St (parquet/csv) no diretório.\")\n",
    "\n",
    "# Conferência rápida\n",
    "assert \"Data\" in df_St.columns, \"df_St precisa ter coluna 'Data'.\"\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 2) Carregar excessos de retorno em t (ex_R_acoes)\n",
    "#    (ou use diretamente o df que você já criou em memória)\n",
    "# ---------------------------------------------\n",
    "ex_path_parquet = Path(\"ex_R_acoes.parquet\")\n",
    "ex_path_csv     = Path(\"ex_R_acoes.csv\")\n",
    "\n",
    "if ex_path_parquet.exists():\n",
    "    ex_R_acoes = pd.read_parquet(ex_path_parquet)\n",
    "elif ex_path_csv.exists():\n",
    "    ex_R_acoes = pd.read_csv(ex_path_csv, parse_dates=[\"Data\"])\n",
    "else:\n",
    "    raise FileNotFoundError(\"Não encontrei ex_R_acoes (parquet/csv) no diretório.\")\n",
    "\n",
    "assert \"Data\" in ex_R_acoes.columns, \"ex_R_acoes precisa ter coluna 'Data'.\"\n",
    "\n",
    "# Ordenar por segurança\n",
    "ex_R_acoes = ex_R_acoes.sort_values(\"Data\").reset_index(drop=True)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 3) Shift de +1 mês para construir alvo t+1\n",
    "#    (trazer ex_R_acoes(t+1) para a linha de t)\n",
    "# ---------------------------------------------\n",
    "# Colunas das ações (todas menos 'Data')\n",
    "action_cols = [c for c in ex_R_acoes.columns if c != \"Data\"]\n",
    "\n",
    "# Copiamos para não mexer no original\n",
    "ex_R_acoes_tmais1 = ex_R_acoes.copy()\n",
    "\n",
    "# Shift de -1 nas colunas das ações:\n",
    "# linha t passa a conter o retorno de t+1\n",
    "ex_R_acoes_tmais1[action_cols] = ex_R_acoes_tmais1[action_cols].shift(-1)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 4) Alinhar com as datas de df_St\n",
    "#    (Data igual à de df_St, valores = excessos de t+1)\n",
    "# ---------------------------------------------\n",
    "df_datas_St = df_St[[\"Data\"]].copy()\n",
    "\n",
    "# Merge por Data; left para manter exatamente as datas de df_St\n",
    "ex_R_acoes_tmais1_treino = df_datas_St.merge(\n",
    "    ex_R_acoes_tmais1,\n",
    "    on=\"Data\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Agora ex_R_acoes_tmais1_treino tem:\n",
    "# - Data (mesmas 110 datas de df_St)\n",
    "# - 114 colunas com ex_R de t+1 (última linha deve ficar NaN)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 5) Checagens\n",
    "# ---------------------------------------------\n",
    "print(ex_R_acoes_tmais1_treino.head())\n",
    "print(ex_R_acoes_tmais1_treino.tail())\n",
    "print(\n",
    "    f\"Shape ex_R_acoes_tmais1_treino: \"\n",
    "    f\"{ex_R_acoes_tmais1_treino.shape[0]} meses × \"\n",
    "    f\"{ex_R_acoes_tmais1_treino.shape[1]-1} ações\"\n",
    ")\n",
    "\n",
    "na_ultima_linha = ex_R_acoes_tmais1_treino.iloc[-1][action_cols].isna().all()\n",
    "print(f\"Última linha com NaN em todas as ações (esperado): {na_ultima_linha}\")\n",
    "\n",
    "# (Opcional) Salvar em disco\n",
    "ex_R_acoes_tmais1_treino.to_parquet(\"ex_R_acoes_tmais1_treino.parquet\", index=False)\n",
    "ex_R_acoes_tmais1_treino.to_csv(\"ex_R_acoes_tmais1_treino.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77698de0",
   "metadata": {},
   "source": [
    "# **4. Mapeando as Janelas de Treino de 12M (Evitar Contaminação)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "3ca3c05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   idx_pred  Data_pred  idx_train_start  idx_train_end\n",
      "0        12 2016-11-30                0             11\n",
      "1        13 2016-12-31                1             12\n",
      "2        14 2017-01-31                2             13\n",
      "3        15 2017-02-28                3             14\n",
      "4        16 2017-03-31                4             15\n",
      "    idx_pred  Data_pred  idx_train_start  idx_train_end\n",
      "92       104 2024-07-31               92            103\n",
      "93       105 2024-08-31               93            104\n",
      "94       106 2024-09-30               94            105\n",
      "95       107 2024-10-31               95            106\n",
      "96       108 2024-11-30               96            107\n",
      "Número de janelas de treino (previsões OOS): 97\n",
      "\n",
      "Primeira janela:\n",
      "  Data_pred          = 2016-11-30\n",
      "  idx_pred           = 12\n",
      "  idx_train_start    = 0\n",
      "  idx_train_end      = 11\n",
      "  Data treino início = 2015-11-30\n",
      "  Data treino fim    = 2016-10-31\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ================================\n",
    "# 1) Carregar df_St (features RFF)\n",
    "# ================================\n",
    "st_path_parquet = Path(\"St_RFF_12000_g2_seed123.parquet\")\n",
    "st_path_csv     = Path(\"St_RFF_12000_g2_seed123.csv\")\n",
    "\n",
    "if st_path_parquet.exists():\n",
    "    df_St = pd.read_parquet(st_path_parquet)\n",
    "elif st_path_csv.exists():\n",
    "    df_St = pd.read_csv(st_path_csv, parse_dates=[\"Data\"])\n",
    "else:\n",
    "    raise FileNotFoundError(\"Não encontrei df_St (parquet/csv) no diretório.\")\n",
    "\n",
    "assert \"Data\" in df_St.columns, \"df_St precisa ter coluna 'Data'.\"\n",
    "df_St = df_St.sort_values(\"Data\").reset_index(drop=True)\n",
    "\n",
    "# ================================\n",
    "# 2) Carregar ex_R_acoes_tmais1_treino (targets t+1)\n",
    "# ================================\n",
    "ex_t1_path_parquet = Path(\"ex_R_acoes_tmais1_treino.parquet\")\n",
    "ex_t1_path_csv     = Path(\"ex_R_acoes_tmais1_treino.csv\")\n",
    "\n",
    "if ex_t1_path_parquet.exists():\n",
    "    ex_R_acoes_tmais1_treino = pd.read_parquet(ex_t1_path_parquet)\n",
    "elif ex_t1_path_csv.exists():\n",
    "    ex_R_acoes_tmais1_treino = pd.read_csv(ex_t1_path_csv, parse_dates=[\"Data\"])\n",
    "else:\n",
    "    raise FileNotFoundError(\"Não encontrei ex_R_acoes_tmais1_treino (parquet/csv) no diretório.\")\n",
    "\n",
    "assert \"Data\" in ex_R_acoes_tmais1_treino.columns, \"ex_R_acoes_tmais1_treino precisa ter coluna 'Data'.\"\n",
    "ex_R_acoes_tmais1_treino = ex_R_acoes_tmais1_treino.sort_values(\"Data\").reset_index(drop=True)\n",
    "\n",
    "# ================================\n",
    "# 3) Garantir alinhamento de datas\n",
    "# ================================\n",
    "# Aqui assumimos que df_St e ex_R_acoes_tmais1_treino têm o mesmo calendário\n",
    "if not df_St[\"Data\"].equals(ex_R_acoes_tmais1_treino[\"Data\"]):\n",
    "    raise ValueError(\"As datas de df_St e ex_R_acoes_tmais1_treino não estão perfeitamente alinhadas.\")\n",
    "\n",
    "datas = df_St[\"Data\"].copy()\n",
    "n_rows = len(datas)\n",
    "\n",
    "# Colunas de ações (targets)\n",
    "action_cols = [c for c in ex_R_acoes_tmais1_treino.columns if c != \"Data\"]\n",
    "\n",
    "# ================================\n",
    "# 4) Construir o \"mapa de janelas\"\n",
    "# ================================\n",
    "janela_treino = 12\n",
    "\n",
    "rows = []\n",
    "for idx_pred in range(n_rows):\n",
    "    # Precisamos de 12 meses ANTES de idx_pred: [idx_pred-12, ..., idx_pred-1]\n",
    "    idx_train_start = idx_pred - janela_treino\n",
    "    idx_train_end   = idx_pred - 1\n",
    "\n",
    "    # Se não tem 12 meses antes, pula (ainda estamos no burn-in)\n",
    "    if idx_train_start < 0:\n",
    "        continue\n",
    "\n",
    "    # Se os targets nessa linha são todos NaN, não nos interessa como previsão\n",
    "    row_targets = ex_R_acoes_tmais1_treino.loc[idx_pred, action_cols]\n",
    "    if row_targets.isna().all():\n",
    "        continue\n",
    "\n",
    "    rows.append({\n",
    "        \"idx_pred\": idx_pred,\n",
    "        \"Data_pred\": datas.iloc[idx_pred],\n",
    "        \"idx_train_start\": idx_train_start,\n",
    "        \"idx_train_end\": idx_train_end\n",
    "    })\n",
    "\n",
    "window_map = pd.DataFrame(rows)\n",
    "\n",
    "# ================================\n",
    "# 5) Checagens\n",
    "# ================================\n",
    "print(window_map.head())\n",
    "print(window_map.tail())\n",
    "print(f\"Número de janelas de treino (previsões OOS): {window_map.shape[0]}\")\n",
    "\n",
    "# Verificar primeira janela\n",
    "first = window_map.iloc[0]\n",
    "print(\"\\nPrimeira janela:\")\n",
    "print(f\"  Data_pred          = {first['Data_pred'].date()}\")\n",
    "print(f\"  idx_pred           = {first['idx_pred']}\")\n",
    "print(f\"  idx_train_start    = {first['idx_train_start']}\")\n",
    "print(f\"  idx_train_end      = {first['idx_train_end']}\")\n",
    "print(f\"  Data treino início = {datas.iloc[first['idx_train_start']].date()}\")\n",
    "print(f\"  Data treino fim    = {datas.iloc[first['idx_train_end']].date()}\")\n",
    "\n",
    "# (Opcional) salvar o mapa de janelas\n",
    "window_map.to_parquet(\"window_map_12m.parquet\", index=False)\n",
    "window_map.to_csv(\"window_map_12m.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "f4faced4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_St: 110 meses × 12000 RFFs\n",
      "ex_R_acoes_tmais1_treino: 110 meses × 114 ações\n",
      "window_map com 97 janelas de treino (previsões OOS)\n",
      "\n",
      "=== Exemplo da primeira janela ===\n",
      "Data_pred: 2016-11-30 00:00:00\n",
      "idx_pred: 12\n",
      "train_slice (start, end): (0, 11)\n",
      "X_train shape: (12, 12000)\n",
      "X_pred shape: (1, 12000)\n",
      "Y_train shape: (12, 114)\n",
      "y_real shape: (1, 114)\n",
      "\n",
      "Média aproximada das RFF padronizadas na janela (deve estar perto de 0): 4.0399782284970975e-19\n",
      "Desvio padrão aproximado das RFF padronizadas na janela (deve estar perto de 1): 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1) Carregar bases principais: df_St, ex_R_acoes_tmais1_treino, window_map\n",
    "# ============================================================\n",
    "\n",
    "# Ajuste os nomes se você salvou com outros\n",
    "st_path_parquet   = Path(\"St_RFF_12000_g2_seed123.parquet\")\n",
    "st_path_csv       = Path(\"St_RFF_12000_g2_seed123.csv\")\n",
    "\n",
    "ex_t1_path_parquet = Path(\"ex_R_acoes_tmais1_treino.parquet\")\n",
    "ex_t1_path_csv     = Path(\"ex_R_acoes_tmais1_treino.csv\")\n",
    "\n",
    "window_map_parquet = Path(\"window_map_12m.parquet\")\n",
    "window_map_csv     = Path(\"window_map_12m.csv\")\n",
    "\n",
    "# --- df_St: RFF brutas ---\n",
    "if st_path_parquet.exists():\n",
    "    df_St = pd.read_parquet(st_path_parquet)\n",
    "elif st_path_csv.exists():\n",
    "    df_St = pd.read_csv(st_path_csv, parse_dates=[\"Data\"])\n",
    "else:\n",
    "    raise FileNotFoundError(\"Não encontrei df_St (parquet/csv). Verifique o caminho/nome do arquivo.\")\n",
    "\n",
    "assert \"Data\" in df_St.columns, \"df_St precisa ter coluna 'Data'.\"\n",
    "df_St = df_St.sort_values(\"Data\").reset_index(drop=True)\n",
    "\n",
    "# --- ex_R_acoes_tmais1_treino: excessos t+1 ---\n",
    "if ex_t1_path_parquet.exists():\n",
    "    ex_R_acoes_tmais1_treino = pd.read_parquet(ex_t1_path_parquet)\n",
    "elif ex_t1_path_csv.exists():\n",
    "    ex_R_acoes_tmais1_treino = pd.read_csv(ex_t1_path_csv, parse_dates=[\"Data\"])\n",
    "else:\n",
    "    raise FileNotFoundError(\"Não encontrei ex_R_acoes_tmais1_treino (parquet/csv).\")\n",
    "\n",
    "assert \"Data\" in ex_R_acoes_tmais1_treino.columns, \"ex_R_acoes_tmais1_treino precisa ter coluna 'Data'.\"\n",
    "ex_R_acoes_tmais1_treino = ex_R_acoes_tmais1_treino.sort_values(\"Data\").reset_index(drop=True)\n",
    "\n",
    "# --- window_map: índices de treino e previsão ---\n",
    "if window_map_parquet.exists():\n",
    "    window_map = pd.read_parquet(window_map_parquet)\n",
    "elif window_map_csv.exists():\n",
    "    window_map = pd.read_csv(window_map_csv, parse_dates=[\"Data_pred\"])\n",
    "else:\n",
    "    raise FileNotFoundError(\"Não encontrei window_map_12m (parquet/csv).\")\n",
    "\n",
    "# ============================================================\n",
    "# 2) Checagens básicas e definição de colunas\n",
    "# ============================================================\n",
    "\n",
    "# Verificar alinhamento total de datas entre df_St e ex_R_acoes_tmais1_treino\n",
    "if not df_St[\"Data\"].equals(ex_R_acoes_tmais1_treino[\"Data\"]):\n",
    "    raise ValueError(\"As datas de df_St e ex_R_acoes_tmais1_treino não estão alinhadas 1:1.\")\n",
    "\n",
    "datas = df_St[\"Data\"].copy()\n",
    "n_rows = len(datas)\n",
    "\n",
    "# Colunas de RFF (features) e de ações (targets)\n",
    "rff_cols = [c for c in df_St.columns if c.startswith(\"RFF_\")]\n",
    "action_cols = [c for c in ex_R_acoes_tmais1_treino.columns if c != \"Data\"]\n",
    "\n",
    "print(f\"df_St: {df_St.shape[0]} meses × {len(rff_cols)} RFFs\")\n",
    "print(f\"ex_R_acoes_tmais1_treino: {ex_R_acoes_tmais1_treino.shape[0]} meses × {len(action_cols)} ações\")\n",
    "print(f\"window_map com {window_map.shape[0]} janelas de treino (previsões OOS)\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3) Função para obter os dados de UMA janela de treino\n",
    "#    Ainda sem Ridge: apenas separa e padroniza X (RFF) por janela\n",
    "# ============================================================\n",
    "\n",
    "def prepare_window_data(\n",
    "    df_St: pd.DataFrame,\n",
    "    ex_R_t1: pd.DataFrame,\n",
    "    window_map: pd.DataFrame,\n",
    "    window_idx: int,\n",
    "    rff_cols: list,\n",
    "    action_cols: list,\n",
    "    std_epsilon: float = 1e-8\n",
    "):\n",
    "    \"\"\"\n",
    "    Prepara os dados de UMA janela de treino (k-ésima linha de window_map).\n",
    "\n",
    "    Retorna um dicionário com:\n",
    "      - 'Data_pred'   : data da previsão (t)\n",
    "      - 'idx_pred'    : índice da linha t\n",
    "      - 'train_slice' : (idx_train_start, idx_train_end)\n",
    "\n",
    "      - 'X_train'     : RFF padronizadas na janela de treino (T_window × P)\n",
    "      - 'X_pred'      : RFF padronizadas no ponto de previsão (1 × P)\n",
    "\n",
    "      - 'Y_train'     : excessos t+1 na janela de treino (T_window × N_ativos)\n",
    "      - 'y_real'      : excesso t+1 realizado no mês previsto (1 × N_ativos)\n",
    "\n",
    "      - 'rff_cols'    : lista de colunas usadas em X\n",
    "      - 'action_cols' : lista de colunas de ações (ordem de Y)\n",
    "    \"\"\"\n",
    "    row = window_map.iloc[window_idx]\n",
    "    idx_pred = int(row[\"idx_pred\"])\n",
    "    idx_train_start = int(row[\"idx_train_start\"])\n",
    "    idx_train_end = int(row[\"idx_train_end\"])\n",
    "    data_pred = row[\"Data_pred\"]\n",
    "\n",
    "    # ---------- Features (RFF) bruto ----------\n",
    "    # Usar .iloc porque idx_* são índices posicionais (0..n-1)\n",
    "    S_window_raw = df_St.loc[idx_train_start:idx_train_end, rff_cols].to_numpy(dtype=float)\n",
    "    S_pred_raw = df_St.loc[[idx_pred], rff_cols].to_numpy(dtype=float)  # matriz 1 × P\n",
    "\n",
    "    # ---------- Targets (excessos t+1) ----------\n",
    "    Y_window = ex_R_t1.loc[idx_train_start:idx_train_end, action_cols].to_numpy(dtype=float)\n",
    "    y_real = ex_R_t1.loc[[idx_pred], action_cols].to_numpy(dtype=float)  # 1 × N_ativos\n",
    "\n",
    "    # ---------- Padronização das RFF na janela ----------\n",
    "    # média e desvio da janela de treino (eixo 0 = colunas)\n",
    "    mean = S_window_raw.mean(axis=0)\n",
    "    std = S_window_raw.std(axis=0, ddof=0)  # ddof=0 (pop), ddof=1 também seria aceitável\n",
    "\n",
    "    # tratar std = 0 (feature constante na janela)\n",
    "    std_safe = np.where(std < std_epsilon, 1.0, std)\n",
    "\n",
    "    # padronizar\n",
    "    X_train = (S_window_raw - mean) / std_safe\n",
    "    X_pred = (S_pred_raw - mean) / std_safe\n",
    "\n",
    "    # Checagens rápidas (opcional, pode comentar depois)\n",
    "    if np.isnan(X_train).any() or np.isinf(X_train).any():\n",
    "        raise ValueError(f\"X_train contém NaN/inf na janela {window_idx}. Verificar.\")\n",
    "    if np.isnan(X_pred).any() or np.isinf(X_pred).any():\n",
    "        raise ValueError(f\"X_pred contém NaN/inf na janela {window_idx}. Verificar.\")\n",
    "\n",
    "    return {\n",
    "        \"Data_pred\": data_pred,\n",
    "        \"idx_pred\": idx_pred,\n",
    "        \"train_slice\": (idx_train_start, idx_train_end),\n",
    "\n",
    "        \"X_train\": X_train,          # (T_window × P)\n",
    "        \"X_pred\": X_pred,            # (1 × P)\n",
    "        \"Y_train\": Y_window,         # (T_window × N_ativos)\n",
    "        \"y_real\": y_real,            # (1 × N_ativos)\n",
    "\n",
    "        \"rff_cols\": rff_cols,\n",
    "        \"action_cols\": action_cols,\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4) (Opcional) Exemplo de uso para verificar se tudo está OK\n",
    "#    Ainda sem Ridge: só inspecionando shapes e estatísticas básicas\n",
    "# ============================================================\n",
    "\n",
    "# Pega a primeira janela como exemplo\n",
    "example_idx = 0\n",
    "example = prepare_window_data(\n",
    "    df_St=df_St,\n",
    "    ex_R_t1=ex_R_acoes_tmais1_treino,\n",
    "    window_map=window_map,\n",
    "    window_idx=example_idx,\n",
    "    rff_cols=rff_cols,\n",
    "    action_cols=action_cols\n",
    ")\n",
    "\n",
    "print(\"\\n=== Exemplo da primeira janela ===\")\n",
    "print(\"Data_pred:\", example[\"Data_pred\"])\n",
    "print(\"idx_pred:\", example[\"idx_pred\"])\n",
    "print(\"train_slice (start, end):\", example[\"train_slice\"])\n",
    "print(\"X_train shape:\", example[\"X_train\"].shape)\n",
    "print(\"X_pred shape:\", example[\"X_pred\"].shape)\n",
    "print(\"Y_train shape:\", example[\"Y_train\"].shape)\n",
    "print(\"y_real shape:\", example[\"y_real\"].shape)\n",
    "\n",
    "# Estatísticas da padronização na janela (só para sanity check)\n",
    "X_train = example[\"X_train\"]\n",
    "print(\"\\nMédia aproximada das RFF padronizadas na janela (deve estar perto de 0):\",\n",
    "      np.mean(X_train))\n",
    "print(\"Desvio padrão aproximado das RFF padronizadas na janela (deve estar perto de 1):\",\n",
    "      np.std(X_train))\n",
    "\n",
    "# ============================================================\n",
    "# A partir daqui, o próximo passo é plugar o Ridge:\n",
    "#   para cada window_idx em range(window_map.shape[0]):\n",
    "#       data = prepare_window_data(...)\n",
    "#       usar data[\"X_train\"], data[\"Y_train\"], data[\"X_pred\"] no Ridge\n",
    "#       e armazenar as previsões + y_real para avaliar a estratégia\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0031ec4",
   "metadata": {},
   "source": [
    "# **5. Estimando os $\\beta$'s (Ridge Regression)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "27fdde69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shrinkage z = 0.1\n",
      "Pasta para salvar betas: C:\\Users\\Hugo Villanova\\Desktop\\DQAI2025\\Metodologia - V3\\Betas_Ridge_z10\n",
      "Iniciando ajuste Ridge em 97 janelas...\n",
      "Janela 10/97 concluída.\n",
      "Janela 20/97 concluída.\n",
      "Janela 30/97 concluída.\n",
      "Janela 40/97 concluída.\n",
      "Janela 50/97 concluída.\n",
      "Janela 60/97 concluída.\n",
      "Janela 70/97 concluída.\n",
      "Janela 80/97 concluída.\n",
      "Janela 90/97 concluída.\n",
      "Janela 97/97 concluída.\n",
      "\n",
      "Ajuste Ridge concluído.\n",
      "Arquivos de betas salvos em: C:\\Users\\Hugo Villanova\\Desktop\\DQAI2025\\Metodologia - V3\\Betas_Ridge_z10\n",
      "Metadados salvos em: C:\\Users\\Hugo Villanova\\Desktop\\DQAI2025\\Metodologia - V3\\Betas_Ridge_z10\\betas_ridge_metadata_z10.csv\n",
      "\n",
      "Pré-visualização dos metadados:\n",
      "   window_idx  Data_pred  idx_pred  idx_train_start  idx_train_end  \\\n",
      "0           0 2016-11-30        12                0             11   \n",
      "1           1 2016-12-31        13                1             12   \n",
      "2           2 2017-01-31        14                2             13   \n",
      "3           3 2017-02-28        15                3             14   \n",
      "4           4 2017-03-31        16                4             15   \n",
      "\n",
      "                       beta_file    z  \n",
      "0  betas_ridge_z0_window_000.npy  0.1  \n",
      "1  betas_ridge_z0_window_001.npy  0.1  \n",
      "2  betas_ridge_z0_window_002.npy  0.1  \n",
      "3  betas_ridge_z0_window_003.npy  0.1  \n",
      "4  betas_ridge_z0_window_004.npy  0.1  \n",
      "\n",
      "Total de janelas com betas salvos: 97\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# ============================================================\n",
    "# 1) Parâmetro de shrinkage e pasta de saída\n",
    "# ============================================================\n",
    "\n",
    "z = 0.1  # parâmetro de shrinkage fixo (L2)\n",
    "betas_dir = Path(\"Betas_Ridge_z10\")\n",
    "os.makedirs(betas_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Shrinkage z = {z}\")\n",
    "print(f\"Pasta para salvar betas: {betas_dir.resolve()}\")\n",
    "\n",
    "# ============================================================\n",
    "# 2) Função de Ridge (forma dual, sem intercepto)\n",
    "# ============================================================\n",
    "\n",
    "def ridge_fit_window(X_train: np.ndarray, Y_train: np.ndarray, z: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Ajusta Ridge multi-output: Y = X B + erro\n",
    "    X_train: (T × P)\n",
    "    Y_train: (T × N)\n",
    "    z      : shrinkage L2\n",
    "\n",
    "    Retorna:\n",
    "      B_hat: (P × N)\n",
    "    \"\"\"\n",
    "    T, P = X_train.shape\n",
    "    _, N = Y_train.shape\n",
    "\n",
    "    # Matriz T×T (XX') e termo de regularização\n",
    "    XXt = X_train @ X_train.T          # (T × T)\n",
    "    A = XXt + z * T * np.eye(T)        # (T × T)\n",
    "\n",
    "    # Resolver A * Alpha = Y para Alpha (T × N)\n",
    "    # Em vez de inverter A, usamos solve\n",
    "    try:\n",
    "        Alpha = np.linalg.solve(A, Y_train)  # (T × N)\n",
    "    except np.linalg.LinAlgError:\n",
    "        # Em caso de problemas numéricos, recorre a pseudo-inversa\n",
    "        Alpha = np.linalg.pinv(A) @ Y_train\n",
    "\n",
    "    # B_hat = X' * Alpha / T  (P × N)\n",
    "    B_hat = (X_train.T @ Alpha) / T\n",
    "    return B_hat\n",
    "\n",
    "# ============================================================\n",
    "# 3) Loop sobre todas as janelas de treino e salvar betas\n",
    "# ============================================================\n",
    "\n",
    "records = []\n",
    "\n",
    "n_windows = window_map.shape[0]\n",
    "print(f\"Iniciando ajuste Ridge em {n_windows} janelas...\")\n",
    "\n",
    "for window_idx in range(n_windows):\n",
    "    # Dados da janela (já com RFF padronizadas!)\n",
    "    data = prepare_window_data(\n",
    "        df_St=df_St,\n",
    "        ex_R_t1=ex_R_acoes_tmais1_treino,\n",
    "        window_map=window_map,\n",
    "        window_idx=window_idx,\n",
    "        rff_cols=rff_cols,\n",
    "        action_cols=action_cols\n",
    "    )\n",
    "\n",
    "    X_train = data[\"X_train\"]    # (12 × 12000)\n",
    "    Y_train = data[\"Y_train\"]    # (12 × 114)\n",
    "\n",
    "    # Ajusta Ridge e obtém B_hat (12000 × 114)\n",
    "    B_hat = ridge_fit_window(X_train, Y_train, z=z)\n",
    "\n",
    "    # Converter para float32 para economizar espaço em disco\n",
    "    B_hat32 = B_hat.astype(np.float32)\n",
    "\n",
    "    # Nome do arquivo de betas desta janela\n",
    "    beta_filename = f\"betas_ridge_z{int(z)}_window_{window_idx:03d}.npy\"\n",
    "    beta_path = betas_dir / beta_filename\n",
    "\n",
    "    # Salvar matriz de betas\n",
    "    np.save(beta_path, B_hat32)\n",
    "\n",
    "    # Guardar metadados da janela\n",
    "    idx_pred = int(data[\"idx_pred\"])\n",
    "    idx_train_start, idx_train_end = data[\"train_slice\"]\n",
    "\n",
    "    records.append({\n",
    "        \"window_idx\": window_idx,\n",
    "        \"Data_pred\": data[\"Data_pred\"],\n",
    "        \"idx_pred\": idx_pred,\n",
    "        \"idx_train_start\": idx_train_start,\n",
    "        \"idx_train_end\": idx_train_end,\n",
    "        \"beta_file\": beta_filename,\n",
    "        \"z\": z\n",
    "    })\n",
    "\n",
    "    if (window_idx + 1) % 10 == 0 or window_idx == n_windows - 1:\n",
    "        print(f\"Janela {window_idx+1}/{n_windows} concluída.\")\n",
    "\n",
    "# ============================================================\n",
    "# 4) Salvar metadados das janelas e checar\n",
    "# ============================================================\n",
    "\n",
    "betas_meta = pd.DataFrame(records)\n",
    "meta_path = betas_dir / \"betas_ridge_metadata_z10.csv\"\n",
    "betas_meta.to_csv(meta_path, index=False)\n",
    "\n",
    "print(\"\\nAjuste Ridge concluído.\")\n",
    "print(f\"Arquivos de betas salvos em: {betas_dir.resolve()}\")\n",
    "print(f\"Metadados salvos em: {meta_path.resolve()}\")\n",
    "\n",
    "print(\"\\nPré-visualização dos metadados:\")\n",
    "print(betas_meta.head())\n",
    "print(f\"\\nTotal de janelas com betas salvos: {betas_meta.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbea14c5",
   "metadata": {},
   "source": [
    "# **6. Previsões OOS $\\hat{R_{t+1}}$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "ba1894b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_St: 110 meses × 12000 RFFs\n",
      "ex_R_acoes_tmais1_treino: 110 meses × 114 ações\n",
      "window_map: 97 janelas\n",
      "betas_meta: 97 janelas\n",
      "Iniciando geração de previsões para 97 janelas...\n",
      "Janela 10/97 processada.\n",
      "Janela 20/97 processada.\n",
      "Janela 30/97 processada.\n",
      "Janela 40/97 processada.\n",
      "Janela 50/97 processada.\n",
      "Janela 60/97 processada.\n",
      "Janela 70/97 processada.\n",
      "Janela 80/97 processada.\n",
      "Janela 90/97 processada.\n",
      "Janela 97/97 processada.\n",
      "\n",
      "Pré-visualização das previsões:\n",
      "   Data_pred     ABCB4     ABEV3     AGRO3     ALPA4    ALUP11     AMER3  \\\n",
      "0 2016-11-30  0.000110 -0.000081  0.000350  0.000099  0.000182  0.000390   \n",
      "1 2016-12-31  0.000097  0.000126 -0.000173 -0.000406 -0.000041  0.000767   \n",
      "2 2017-01-31  0.000147 -0.000020 -0.000030 -0.000132  0.000307  0.000462   \n",
      "3 2017-02-28 -0.000061 -0.000055 -0.000113  0.000040 -0.000370 -0.000242   \n",
      "4 2017-03-31 -0.000195  0.000103  0.000046 -0.000117 -0.000166  0.000151   \n",
      "\n",
      "      ANIM3     AZZA3     B3SA3  ...     TUPY3     UCAS3     UGPA3     UNIP6  \\\n",
      "0  0.000660  0.000035  0.000163  ...  0.000189  0.000407  0.000114  0.000148   \n",
      "1 -0.000751 -0.000431 -0.000311  ...  0.000579  0.000339  0.000015  0.000020   \n",
      "2  0.000248 -0.000030  0.000122  ...  0.000261  0.000126  0.000087 -0.000047   \n",
      "3 -0.000451 -0.000128 -0.000471  ...  0.000222 -0.000477 -0.000205  0.000039   \n",
      "4 -0.000396 -0.000060 -0.000151  ... -0.000079  0.000293 -0.000090 -0.000036   \n",
      "\n",
      "      USIM3     USIM5     VALE3     VLID3     WEGE3     YDUQ3  \n",
      "0  0.000526  0.001130  0.000545  0.000088 -0.000073  0.000456  \n",
      "1 -0.000192 -0.000986 -0.001088 -0.000171  0.000167 -0.000396  \n",
      "2 -0.000105  0.000279 -0.000045  0.000129  0.000048  0.000410  \n",
      "3  0.000027 -0.001043 -0.000476 -0.000284 -0.000103 -0.000780  \n",
      "4  0.000764  0.000323 -0.000023  0.000178  0.000165 -0.000097  \n",
      "\n",
      "[5 rows x 115 columns]\n",
      "\n",
      "Pré-visualização dos realizados (excessos t+1):\n",
      "   Data_pred     ABCB4     ABEV3     AGRO3     ALPA4    ALUP11     AMER3  \\\n",
      "0 2016-11-30 -0.001779 -0.042317  0.089691 -0.034848  0.029423 -0.116221   \n",
      "1 2016-12-31  0.210103  0.040886  0.063321  0.026472  0.066282  0.151732   \n",
      "2 2017-01-31  0.086434  0.027933 -0.000233  0.120348  0.065468  0.016092   \n",
      "3 2017-02-28  0.018256  0.011725 -0.012117  0.036434  0.018782  0.017257   \n",
      "4 2017-03-31 -0.057627 -0.006726  0.057603  0.070343 -0.063916  0.072283   \n",
      "\n",
      "      ANIM3     AZZA3     B3SA3  ...     TUPY3     UCAS3     UGPA3     UNIP6  \\\n",
      "0 -0.005268 -0.093670 -0.004497  ...  0.023579  0.114533 -0.020606 -0.002357   \n",
      "1 -0.027806  0.138678  0.103082  ... -0.018309  0.075896 -0.044815  0.010874   \n",
      "2  0.020088  0.070331  0.019660  ...  0.140021 -0.012218 -0.020155  0.053703   \n",
      "3 -0.039138 -0.013949  0.003646  ...  0.053954 -0.228173  0.094004 -0.001097   \n",
      "4 -0.003220  0.063505 -0.022444  ...  0.033192 -0.102232 -0.024719  0.024882   \n",
      "\n",
      "      USIM3     USIM5     VALE3     VLID3     WEGE3     YDUQ3  \n",
      "0 -0.077886 -0.023276 -0.093930 -0.030911 -0.010278 -0.039964  \n",
      "1 -0.009577  0.236454  0.214845 -0.037949  0.015321 -0.004478  \n",
      "2  0.075989 -0.071490  0.008040  0.064970  0.056487 -0.067537  \n",
      "3 -0.081826 -0.115134 -0.104263 -0.142874  0.021621  0.046598  \n",
      "4  0.038806 -0.049207 -0.056084 -0.014482  0.009272  0.125854  \n",
      "\n",
      "[5 rows x 115 columns]\n",
      "\n",
      "df_preds shape: (97, 115) (meses OOS × (1 + 114 ações))\n",
      "df_real  shape: (97, 115)\n",
      "\n",
      "Previsões salvas em: C:\\Users\\Hugo Villanova\\Desktop\\DQAI2025\\Metodologia - V3\\predicoes_excesso_ridge_z10.parquet\n",
      "Realizados salvos em: C:\\Users\\Hugo Villanova\\Desktop\\DQAI2025\\Metodologia - V3\\reais_excesso_ridge_z10.parquet\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ============================================================\n",
    "# 1) Caminhos dos arquivos\n",
    "# ============================================================\n",
    "\n",
    "st_path_parquet    = Path(\"St_RFF_12000_g2_seed123.parquet\")\n",
    "st_path_csv        = Path(\"St_RFF_12000_g2_seed123.csv\")\n",
    "\n",
    "ex_t1_path_parquet = Path(\"ex_R_acoes_tmais1_treino.parquet\")\n",
    "ex_t1_path_csv     = Path(\"ex_R_acoes_tmais1_treino.csv\")\n",
    "\n",
    "window_map_parquet = Path(\"window_map_12m.parquet\")\n",
    "window_map_csv     = Path(\"window_map_12m.csv\")\n",
    "\n",
    "betas_dir          = Path(\"Betas_Ridge_z10\")\n",
    "betas_meta_path    = betas_dir / \"betas_ridge_metadata_z10.csv\"\n",
    "\n",
    "# ============================================================\n",
    "# 2) Carregar bases principais\n",
    "# ============================================================\n",
    "\n",
    "# --- df_St: RFF brutas ---\n",
    "if st_path_parquet.exists():\n",
    "    df_St = pd.read_parquet(st_path_parquet)\n",
    "elif st_path_csv.exists():\n",
    "    df_St = pd.read_csv(st_path_csv, parse_dates=[\"Data\"])\n",
    "else:\n",
    "    raise FileNotFoundError(\"Não encontrei df_St (parquet/csv).\")\n",
    "\n",
    "assert \"Data\" in df_St.columns, \"df_St precisa ter coluna 'Data'.\"\n",
    "df_St = df_St.sort_values(\"Data\").reset_index(drop=True)\n",
    "\n",
    "# --- ex_R_acoes_tmais1_treino: excessos t+1 ---\n",
    "if ex_t1_path_parquet.exists():\n",
    "    ex_R_acoes_tmais1_treino = pd.read_parquet(ex_t1_path_parquet)\n",
    "elif ex_t1_path_csv.exists():\n",
    "    ex_R_acoes_tmais1_treino = pd.read_csv(ex_t1_path_csv, parse_dates=[\"Data\"])\n",
    "else:\n",
    "    raise FileNotFoundError(\"Não encontrei ex_R_acoes_tmais1_treino (parquet/csv).\")\n",
    "\n",
    "assert \"Data\" in ex_R_acoes_tmais1_treino.columns, \"ex_R_acoes_tmais1_treino precisa ter coluna 'Data'.\"\n",
    "ex_R_acoes_tmais1_treino = ex_R_acoes_tmais1_treino.sort_values(\"Data\").reset_index(drop=True)\n",
    "\n",
    "# --- window_map: índices de treino e previsão ---\n",
    "if window_map_parquet.exists():\n",
    "    window_map = pd.read_parquet(window_map_parquet)\n",
    "elif window_map_csv.exists():\n",
    "    window_map = pd.read_csv(window_map_csv, parse_dates=[\"Data_pred\"])\n",
    "else:\n",
    "    raise FileNotFoundError(\"Não encontrei window_map_12m (parquet/csv).\")\n",
    "\n",
    "# --- metadados dos betas ---\n",
    "if not betas_meta_path.exists():\n",
    "    raise FileNotFoundError(f\"Metadados de betas não encontrados em {betas_meta_path}\")\n",
    "\n",
    "betas_meta = pd.read_csv(betas_meta_path, parse_dates=[\"Data_pred\"])\n",
    "\n",
    "# ============================================================\n",
    "# 3) Checagens básicas e definição de colunas\n",
    "# ============================================================\n",
    "\n",
    "# Verificar alinhamento de datas entre df_St e ex_R_acoes_tmais1_treino\n",
    "if not df_St[\"Data\"].equals(ex_R_acoes_tmais1_treino[\"Data\"]):\n",
    "    raise ValueError(\"As datas de df_St e ex_R_acoes_tmais1_treino não estão alinhadas 1:1.\")\n",
    "\n",
    "datas = df_St[\"Data\"].copy()\n",
    "\n",
    "# Colunas de RFF (features) e ações (targets)\n",
    "rff_cols = [c for c in df_St.columns if c.startswith(\"RFF_\")]\n",
    "action_cols = [c for c in ex_R_acoes_tmais1_treino.columns if c != \"Data\"]\n",
    "\n",
    "print(f\"df_St: {df_St.shape[0]} meses × {len(rff_cols)} RFFs\")\n",
    "print(f\"ex_R_acoes_tmais1_treino: {ex_R_acoes_tmais1_treino.shape[0]} meses × {len(action_cols)} ações\")\n",
    "print(f\"window_map: {window_map.shape[0]} janelas\")\n",
    "print(f\"betas_meta: {betas_meta.shape[0]} janelas\")\n",
    "\n",
    "# Garantir que window_map e betas_meta têm mesma quantidade de janelas\n",
    "if window_map.shape[0] != betas_meta.shape[0]:\n",
    "    raise ValueError(\"window_map e betas_meta têm números de janelas diferentes.\")\n",
    "\n",
    "# ============================================================\n",
    "# 4) (Re)definir prepare_window_data se não estiver no escopo\n",
    "#    (SE você já tiver ela no notebook, pode pular esta definição)\n",
    "# ============================================================\n",
    "\n",
    "def prepare_window_data(\n",
    "    df_St: pd.DataFrame,\n",
    "    ex_R_t1: pd.DataFrame,\n",
    "    window_map: pd.DataFrame,\n",
    "    window_idx: int,\n",
    "    rff_cols: list,\n",
    "    action_cols: list,\n",
    "    std_epsilon: float = 1e-8\n",
    "):\n",
    "    row = window_map.iloc[window_idx]\n",
    "    idx_pred = int(row[\"idx_pred\"])\n",
    "    idx_train_start = int(row[\"idx_train_start\"])\n",
    "    idx_train_end = int(row[\"idx_train_end\"])\n",
    "    data_pred = row[\"Data_pred\"]\n",
    "\n",
    "    # Features bruto\n",
    "    S_window_raw = df_St.loc[idx_train_start:idx_train_end, rff_cols].to_numpy(dtype=float)\n",
    "    S_pred_raw   = df_St.loc[[idx_pred], rff_cols].to_numpy(dtype=float)\n",
    "\n",
    "    # Targets (t+1) bruto\n",
    "    Y_window = ex_R_t1.loc[idx_train_start:idx_train_end, action_cols].to_numpy(dtype=float)\n",
    "    y_real   = ex_R_t1.loc[[idx_pred], action_cols].to_numpy(dtype=float)\n",
    "\n",
    "    # Padronização das RFF na janela\n",
    "    mean = S_window_raw.mean(axis=0)\n",
    "    std  = S_window_raw.std(axis=0, ddof=0)\n",
    "    std_safe = np.where(std < std_epsilon, 1.0, std)\n",
    "\n",
    "    X_train = (S_window_raw - mean) / std_safe\n",
    "    X_pred  = (S_pred_raw  - mean) / std_safe\n",
    "\n",
    "    return {\n",
    "        \"Data_pred\": data_pred,\n",
    "        \"idx_pred\": idx_pred,\n",
    "        \"train_slice\": (idx_train_start, idx_train_end),\n",
    "        \"X_train\": X_train,\n",
    "        \"X_pred\": X_pred,\n",
    "        \"Y_train\": Y_window,\n",
    "        \"y_real\": y_real,\n",
    "        \"rff_cols\": rff_cols,\n",
    "        \"action_cols\": action_cols,\n",
    "    }\n",
    "\n",
    "# ============================================================\n",
    "# 5) Loop para gerar previsões OOS (sem ranking)\n",
    "# ============================================================\n",
    "\n",
    "all_preds = []   # para guardar previsões\n",
    "all_real  = []   # para guardar realizados, se quiser\n",
    "\n",
    "n_windows = window_map.shape[0]\n",
    "print(f\"Iniciando geração de previsões para {n_windows} janelas...\")\n",
    "\n",
    "for window_idx in range(n_windows):\n",
    "    # Dados da janela (X_pred padronizado + y_real)\n",
    "    data = prepare_window_data(\n",
    "        df_St=df_St,\n",
    "        ex_R_t1=ex_R_acoes_tmais1_treino,\n",
    "        window_map=window_map,\n",
    "        window_idx=window_idx,\n",
    "        rff_cols=rff_cols,\n",
    "        action_cols=action_cols\n",
    "    )\n",
    "\n",
    "    X_pred = data[\"X_pred\"]   # (1 × 12000)\n",
    "    y_real = data[\"y_real\"]   # (1 × 114)\n",
    "    data_pred = data[\"Data_pred\"]\n",
    "\n",
    "    # Arquivo de betas correspondente à janela\n",
    "    beta_file = betas_meta.loc[betas_meta[\"window_idx\"] == window_idx, \"beta_file\"].iloc[0]\n",
    "    beta_path = betas_dir / beta_file\n",
    "\n",
    "    if not beta_path.exists():\n",
    "        raise FileNotFoundError(f\"Arquivo de betas não encontrado: {beta_path}\")\n",
    "\n",
    "    # Carregar B_hat (12000 × 114)\n",
    "    B_hat = np.load(beta_path)  # foi salvo em float32\n",
    "\n",
    "    # Previsão: y_hat = X_pred @ B_hat  → shape (1 × 114)\n",
    "    y_hat = X_pred @ B_hat\n",
    "    y_hat = y_hat.reshape(-1)   # vetor 1D com 114 elementos\n",
    "    y_real_vec = y_real.reshape(-1)\n",
    "\n",
    "    # Montar dict com previsões e realizados\n",
    "    pred_row = {\"Data_pred\": data_pred}\n",
    "    real_row = {\"Data_pred\": data_pred}\n",
    "\n",
    "    # Preencher com colunas por ação (mesma ordem de action_cols)\n",
    "    for col_name, val_pred, val_real in zip(action_cols, y_hat, y_real_vec):\n",
    "        pred_row[col_name] = val_pred\n",
    "        real_row[col_name] = val_real\n",
    "\n",
    "    all_preds.append(pred_row)\n",
    "    all_real.append(real_row)\n",
    "\n",
    "    if (window_idx + 1) % 10 == 0 or window_idx == n_windows - 1:\n",
    "        print(f\"Janela {window_idx+1}/{n_windows} processada.\")\n",
    "\n",
    "# ============================================================\n",
    "# 6) Construir DataFrames de previsões e realizados e salvar\n",
    "# ============================================================\n",
    "\n",
    "df_preds = pd.DataFrame(all_preds).sort_values(\"Data_pred\").reset_index(drop=True)\n",
    "df_real  = pd.DataFrame(all_real).sort_values(\"Data_pred\").reset_index(drop=True)\n",
    "\n",
    "print(\"\\nPré-visualização das previsões:\")\n",
    "print(df_preds.head())\n",
    "\n",
    "print(\"\\nPré-visualização dos realizados (excessos t+1):\")\n",
    "print(df_real.head())\n",
    "\n",
    "print(f\"\\ndf_preds shape: {df_preds.shape} (meses OOS × (1 + {len(action_cols)} ações))\")\n",
    "print(f\"df_real  shape: {df_real.shape}\")\n",
    "\n",
    "# Salvar em disco\n",
    "preds_path_parquet = Path(\"predicoes_excesso_ridge_z10.parquet\")\n",
    "real_path_parquet  = Path(\"reais_excesso_ridge_z10.parquet\")\n",
    "\n",
    "df_preds.to_parquet(preds_path_parquet, index=False)\n",
    "df_real.to_parquet(real_path_parquet, index=False)\n",
    "\n",
    "# (Opcional) também em CSV\n",
    "df_preds.to_csv(\"predicoes_excesso_ridge_z10.csv\", index=False)\n",
    "df_real.to_csv(\"reais_excesso_ridge_z10.csv\", index=False)\n",
    "\n",
    "print(f\"\\nPrevisões salvas em: {preds_path_parquet.resolve()}\")\n",
    "print(f\"Realizados salvos em: {real_path_parquet.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27c7e44",
   "metadata": {},
   "source": [
    "# **7. Ranking Long-Short**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "437a048b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meses OOS: 97, número de ações: 114\n",
      "   Data_pred  Ticker  Side  Pred_excess  Real_excess\n",
      "0 2016-11-30   BRAP4  LONG     0.000717    -0.010085\n",
      "1 2016-11-30   CSNA3  LONG     0.000681    -0.147103\n",
      "2 2016-11-30   GOAU4  LONG     0.000870    -0.207270\n",
      "3 2016-11-30  GOLL54  LONG     0.001079    -0.170856\n",
      "4 2016-11-30   OIBR3  LONG     0.000699    -0.055772\n",
      "\n",
      "Total de linhas (meses × 20 ativos): 1940\n",
      "\n",
      "Arquivo Excel salvo em: C:\\Users\\Hugo Villanova\\Desktop\\DQAI2025\\Metodologia - V3\\sinais_long_short_ridge_z10.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ===========================================\n",
    "# 1) Carregar previsões e realizados\n",
    "# ===========================================\n",
    "preds_parquet = Path(\"predicoes_excesso_ridge_z10.parquet\")\n",
    "preds_csv     = Path(\"predicoes_excesso_ridge_z10.csv\")\n",
    "\n",
    "real_parquet  = Path(\"reais_excesso_ridge_z10.parquet\")\n",
    "real_csv      = Path(\"reais_excesso_ridge_z10.csv\")\n",
    "\n",
    "# df_preds: Data_pred + 114 colunas de previsões\n",
    "if preds_parquet.exists():\n",
    "    df_preds = pd.read_parquet(preds_parquet)\n",
    "elif preds_csv.exists():\n",
    "    df_preds = pd.read_csv(preds_csv, parse_dates=[\"Data_pred\"])\n",
    "else:\n",
    "    raise FileNotFoundError(\"Não encontrei predicoes_excesso_ridge_z10 (parquet/csv).\")\n",
    "\n",
    "# df_real: Data_pred + 114 colunas de realizados\n",
    "if real_parquet.exists():\n",
    "    df_real = pd.read_parquet(real_parquet)\n",
    "elif real_csv.exists():\n",
    "    df_real = pd.read_csv(real_csv, parse_dates=[\"Data_pred\"])\n",
    "else:\n",
    "    raise FileNotFoundError(\"Não encontrei reais_excesso_ridge_z10 (parquet/csv).\")\n",
    "\n",
    "# Garantir que as datas estão alinhadas\n",
    "df_preds = df_preds.sort_values(\"Data_pred\").reset_index(drop=True)\n",
    "df_real  = df_real.sort_values(\"Data_pred\").reset_index(drop=True)\n",
    "\n",
    "if not df_preds[\"Data_pred\"].equals(df_real[\"Data_pred\"]):\n",
    "    raise ValueError(\"Datas de df_preds e df_real não estão alinhadas.\")\n",
    "\n",
    "# Colunas de ações (todas menos Data_pred)\n",
    "action_cols = [c for c in df_preds.columns if c != \"Data_pred\"]\n",
    "\n",
    "print(f\"Meses OOS: {df_preds.shape[0]}, número de ações: {len(action_cols)}\")\n",
    "\n",
    "# ===========================================\n",
    "# 2) Para cada mês, selecionar top 10 e bottom 10\n",
    "# ===========================================\n",
    "rows = []\n",
    "\n",
    "for i in range(df_preds.shape[0]):\n",
    "    data_pred = df_preds.loc[i, \"Data_pred\"]\n",
    "    \n",
    "    # Séries de previsões e realizados nesse mês (índice = ticker)\n",
    "    preds_row = df_preds.loc[i, action_cols]\n",
    "    real_row  = df_real.loc[i, action_cols]\n",
    "    \n",
    "    # Ordenar por previsão (descrescente)\n",
    "    preds_sorted = preds_row.sort_values(ascending=False)\n",
    "    \n",
    "    # Top 10 LONG\n",
    "    longs = preds_sorted.index[:10]\n",
    "    # Bottom 10 SHORT\n",
    "    shorts = preds_sorted.index[-10:]\n",
    "    \n",
    "    # Registrar LONGs\n",
    "    for ticker in longs:\n",
    "        rows.append({\n",
    "            \"Data_pred\": data_pred,\n",
    "            \"Ticker\": ticker,\n",
    "            \"Side\": \"LONG\",\n",
    "            \"Pred_excess\": preds_row[ticker],\n",
    "            \"Real_excess\": real_row[ticker],\n",
    "        })\n",
    "    \n",
    "    # Registrar SHORTs\n",
    "    for ticker in shorts:\n",
    "        rows.append({\n",
    "            \"Data_pred\": data_pred,\n",
    "            \"Ticker\": ticker,\n",
    "            \"Side\": \"SHORT\",\n",
    "            \"Pred_excess\": preds_row[ticker],\n",
    "            \"Real_excess\": real_row[ticker],\n",
    "        })\n",
    "\n",
    "# ===========================================\n",
    "# 3) Construir DataFrame final e salvar em xlsx\n",
    "# ===========================================\n",
    "df_signals = pd.DataFrame(rows)\n",
    "\n",
    "# Ordenar por data e, dentro da data, LONGs em cima, depois SHORTs\n",
    "side_order = {\"LONG\": 0, \"SHORT\": 1}\n",
    "df_signals[\"Side_order\"] = df_signals[\"Side\"].map(side_order)\n",
    "\n",
    "df_signals = (\n",
    "    df_signals\n",
    "    .sort_values([\"Data_pred\", \"Side_order\", \"Ticker\"])\n",
    "    .drop(columns=[\"Side_order\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(df_signals.head())\n",
    "print(f\"\\nTotal de linhas (meses × 20 ativos): {df_signals.shape[0]}\")\n",
    "\n",
    "# Salvar em Excel\n",
    "output_path = Path(\"sinais_long_short_ridge_z10.xlsx\")\n",
    "df_signals.to_excel(output_path, index=False)\n",
    "\n",
    "print(f\"\\nArquivo Excel salvo em: {output_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce359e5",
   "metadata": {},
   "source": [
    "# **8. Retornos Finais**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "6306e927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custo de transação por rebalance (simples): 0.1000%\n",
      "   Data_pred Ticker   Side  Pred_excess  Real_excess\n",
      "0 2016-11-30  BRAP4   LONG     0.000717    -0.010085\n",
      "1 2016-11-30  WEGE3  SHORT    -0.000073    -0.010278\n",
      "2 2016-11-30  TOTS3  SHORT    -0.000059     0.110706\n",
      "3 2016-11-30  SMTO3  SHORT    -0.000252     0.071484\n",
      "4 2016-11-30  SIMH3  SHORT    -0.000311    -0.088281\n",
      "Datas diárias de ações: 2013-12-02 → 2024-12-30\n",
      "Datas diárias de CDI:   2013-12-02 → 2024-12-30\n",
      "Número de ações na base diária: 281\n",
      "    Data_dia  Data_pred   Mes_t1  Ret_log_LONG  Ret_log_SHORT  \\\n",
      "0 2016-12-01 2016-11-30  2016-12     -0.042912       0.030719   \n",
      "1 2016-12-02 2016-11-30  2016-12      0.000579      -0.002598   \n",
      "2 2016-12-05 2016-11-30  2016-12     -0.003114       0.011240   \n",
      "3 2016-12-06 2016-11-30  2016-12      0.021293      -0.021410   \n",
      "4 2016-12-07 2016-11-30  2016-12      0.004438      -0.003579   \n",
      "\n",
      "   Ret_log_TOTAL_bruto  Ret_log_TOTAL_liq  Ret_log_CDI_LogRet_D  \n",
      "0            -0.010355          -0.011366              0.000507  \n",
      "1            -0.001509          -0.001509              0.000507  \n",
      "2             0.008664           0.008664              0.000507  \n",
      "3             0.000845           0.000845              0.000507  \n",
      "4             0.001381           0.001381              0.000507  \n",
      "\n",
      "Total de dias na série da estratégia: 2005\n",
      "\n",
      "Arquivo de retornos diários salvo em: C:\\Users\\Hugo Villanova\\Desktop\\DQAI2025\\Metodologia - V3\\retornos_diarios_estrategia_z10_comCDI_e_custo.xlsx\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ====================================================\n",
    "# 1) Caminhos dos arquivos\n",
    "# ====================================================\n",
    "\n",
    "base_path   = Path(\"Base_Final_DQAI2025_v3.xlsx\")\n",
    "sinais_path = Path(\"sinais_long_short_ridge_z10.xlsx\")\n",
    "\n",
    "# Parâmetro de custo de transação\n",
    "# custo por unidade de exposição bruta (long + short) por rebalance\n",
    "TX_COST_RATE = 0.0005  # 5 bps\n",
    "GROSS_EXPOSURE = 2.0   # 100% long + 100% short => exposição bruta = 2\n",
    "TX_COST_SIMPLE = TX_COST_RATE * GROSS_EXPOSURE  # custo simples aplicado no 1º dia útil de t+1\n",
    "\n",
    "print(f\"Custo de transação por rebalance (simples): {TX_COST_SIMPLE:.4%}\")\n",
    "\n",
    "# ====================================================\n",
    "# 2) Carregar sinais mensais (LONG/SHORT)\n",
    "# ====================================================\n",
    "\n",
    "df_sinais = pd.read_excel(sinais_path)\n",
    "df_sinais[\"Data_pred\"] = pd.to_datetime(df_sinais[\"Data_pred\"])\n",
    "\n",
    "# Garantir ordenação por data\n",
    "df_sinais = df_sinais.sort_values(\"Data_pred\").reset_index(drop=True)\n",
    "\n",
    "print(df_sinais.head())\n",
    "\n",
    "# ====================================================\n",
    "# 3) Carregar log-retornos diários das ações e CDI\n",
    "# ====================================================\n",
    "\n",
    "# Ações - log retornos diários\n",
    "df_retD = pd.read_excel(base_path, sheet_name=\"Acoes Log Ret D\")\n",
    "df_retD[\"Data\"] = pd.to_datetime(df_retD[\"Data\"], dayfirst=True, errors=\"coerce\")\n",
    "df_retD = df_retD.dropna(subset=[\"Data\"]).sort_values(\"Data\").reset_index(drop=True)\n",
    "\n",
    "# CDI - log retornos diários\n",
    "df_cdiD = pd.read_excel(base_path, sheet_name=\"CDI Log Ret D\")\n",
    "df_cdiD[\"Data\"] = pd.to_datetime(df_cdiD[\"Data\"], dayfirst=True, errors=\"coerce\")\n",
    "df_cdiD = df_cdiD.dropna(subset=[\"Data\"]).sort_values(\"Data\").reset_index(drop=True)\n",
    "\n",
    "# Identificar coluna de CDI\n",
    "cdi_cols = [c for c in df_cdiD.columns if c != \"Data\"]\n",
    "assert len(cdi_cols) >= 1, \"Sheet 'CDI Log Ret D' deve ter pelo menos 1 coluna além de 'Data'.\"\n",
    "cdi_col = cdi_cols[0]\n",
    "\n",
    "# Criar coluna de Year-Month (período) para facilitar filtro por mês\n",
    "df_retD[\"YM\"] = df_retD[\"Data\"].dt.to_period(\"M\")\n",
    "df_cdiD[\"YM\"] = df_cdiD[\"Data\"].dt.to_period(\"M\")\n",
    "\n",
    "# Lista de tickers disponíveis na base diária\n",
    "tickers_diarios = [c for c in df_retD.columns if c not in [\"Data\", \"YM\"]]\n",
    "\n",
    "print(f\"Datas diárias de ações: {df_retD['Data'].min().date()} → {df_retD['Data'].max().date()}\")\n",
    "print(f\"Datas diárias de CDI:   {df_cdiD['Data'].min().date()} → {df_cdiD['Data'].max().date()}\")\n",
    "print(f\"Número de ações na base diária: {len(tickers_diarios)}\")\n",
    "\n",
    "# ====================================================\n",
    "# 4) Loop por mês OOS: montar retornos diários da estratégia\n",
    "# ====================================================\n",
    "\n",
    "rows_out = []\n",
    "\n",
    "# Agrupar sinais por Data_pred (cada mês OOS)\n",
    "for data_pred, grupo in df_sinais.groupby(\"Data_pred\"):\n",
    "    # Mês \"alvo\" é t+1 (mês seguinte ao Data_pred)\n",
    "    ym_pred = data_pred.to_period(\"M\")\n",
    "    ym_target = ym_pred + 1  # mês t+1\n",
    "\n",
    "    # Filtrar datas diárias no mês t+1\n",
    "    mask_ret = df_retD[\"YM\"] == ym_target\n",
    "    df_mes_ret = df_retD.loc[mask_ret].copy()\n",
    "\n",
    "    mask_cdi = df_cdiD[\"YM\"] == ym_target\n",
    "    df_mes_cdi = df_cdiD.loc[mask_cdi, [\"Data\", cdi_col]].copy()\n",
    "\n",
    "    if df_mes_ret.empty or df_mes_cdi.empty:\n",
    "        print(f\"Atenção: sem dados diários para o mês {ym_target} (Data_pred = {data_pred.date()}).\")\n",
    "        continue\n",
    "\n",
    "    # Garantir alinhamento de dias entre ações e CDI\n",
    "    df_mes_ret = df_mes_ret.set_index(\"Data\")\n",
    "    df_mes_cdi = df_mes_cdi.set_index(\"Data\")\n",
    "    df_mes = df_mes_ret.join(df_mes_cdi, how=\"inner\").sort_index()\n",
    "\n",
    "    if df_mes.empty:\n",
    "        print(f\"Atenção: sem interseção de dias entre ações e CDI para o mês {ym_target}.\")\n",
    "        continue\n",
    "\n",
    "    # Tickers LONG e SHORT nesse Data_pred\n",
    "    longs  = grupo.loc[grupo[\"Side\"] == \"LONG\", \"Ticker\"].tolist()\n",
    "    shorts = grupo.loc[grupo[\"Side\"] == \"SHORT\", \"Ticker\"].tolist()\n",
    "\n",
    "    if len(longs) != 10 or len(shorts) != 10:\n",
    "        print(f\"Atenção: Data_pred = {data_pred.date()} não tem 10 LONG e 10 SHORT (LONG={len(longs)}, SHORT={len(shorts)}).\")\n",
    "\n",
    "    longs_valid  = [t for t in longs if t in df_mes.columns]\n",
    "    shorts_valid = [t for t in shorts if t in df_mes.columns]\n",
    "\n",
    "    if len(longs_valid) == 0 or len(shorts_valid) == 0:\n",
    "        print(f\"Atenção: nenhum ticker válido em LONG ou SHORT para Data_pred = {data_pred.date()}. Pulando.\")\n",
    "        continue\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # 4.1. Converter log-retornos das ações para simples\n",
    "    # ----------------------------------------------------\n",
    "    # Long leg\n",
    "    log_ret_long_assets = df_mes[longs_valid]              # DataFrame (dias × n_long)\n",
    "    r_long_assets = np.exp(log_ret_long_assets) - 1.0      # simples\n",
    "\n",
    "    # Short leg\n",
    "    log_ret_short_assets = df_mes[shorts_valid]            # DataFrame (dias × n_short)\n",
    "    r_short_assets = np.exp(log_ret_short_assets) - 1.0    # simples\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # 4.2. Retornos simples das carteiras LONG e SHORT\n",
    "    #      (equal-weight: média simples das ações em cada dia)\n",
    "    # ----------------------------------------------------\n",
    "    r_long = r_long_assets.mean(axis=1)            # série diária (simples)\n",
    "    r_short_underlying = r_short_assets.mean(axis=1)\n",
    "\n",
    "    # Perna short: lucramos quando o ativo cai ⇒ retorno = -retorno do ativo\n",
    "    r_short = -r_short_underlying\n",
    "\n",
    "    # CDI em retorno simples\n",
    "    r_cdi_simple = np.exp(df_mes[cdi_col]) - 1.0\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # 4.3. Retorno simples TOTAL da estratégia:\n",
    "    #      Long + Short + CDI (caixa do short rendendo CDI)\n",
    "    # ----------------------------------------------------\n",
    "    r_total_bruto = r_long + r_short + r_cdi_simple\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # 4.4. Aplicar custo de transação no primeiro dia útil do mês t+1\n",
    "    # ----------------------------------------------------\n",
    "    r_total_liq = r_total_bruto.copy()\n",
    "\n",
    "    # Índice do primeiro dia útil desse mês (primeira linha de df_mes)\n",
    "    first_day = df_mes.index[0]\n",
    "    # Aplicar custo simples nesse dia (subtraindo TX_COST_SIMPLE)\n",
    "    r_total_liq.loc[first_day] = r_total_liq.loc[first_day] - TX_COST_SIMPLE\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # 4.5. Converter tudo para log-retornos\n",
    "    # ----------------------------------------------------\n",
    "    ret_long_log      = np.log1p(r_long)\n",
    "    ret_short_log     = np.log1p(r_short)\n",
    "    ret_total_bruto_log = np.log1p(r_total_bruto)\n",
    "    ret_total_liq_log   = np.log1p(r_total_liq)\n",
    "    cdi_log           = df_mes[cdi_col]  # já em log\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # 4.6. Guardar resultados dia a dia\n",
    "    # ----------------------------------------------------\n",
    "    for data_dia in df_mes.index:\n",
    "        rows_out.append({\n",
    "            \"Data_dia\": data_dia,\n",
    "            \"Data_pred\": data_pred,         # mês em que a carteira foi formada\n",
    "            \"Mes_t1\": ym_target,            # mês de referência dos retornos\n",
    "            \"Ret_log_LONG\":     ret_long_log.loc[data_dia],\n",
    "            \"Ret_log_SHORT\":    ret_short_log.loc[data_dia],\n",
    "            \"Ret_log_TOTAL_bruto\": ret_total_bruto_log.loc[data_dia],\n",
    "            \"Ret_log_TOTAL_liq\":   ret_total_liq_log.loc[data_dia],\n",
    "            f\"Ret_log_{cdi_col}\": cdi_log.loc[data_dia],\n",
    "        })\n",
    "\n",
    "# ====================================================\n",
    "# 5) Construir DataFrame final e salvar em Excel\n",
    "# ====================================================\n",
    "\n",
    "df_ret_diario = pd.DataFrame(rows_out)\n",
    "df_ret_diario = (\n",
    "    df_ret_diario\n",
    "    .sort_values([\"Data_dia\", \"Data_pred\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(df_ret_diario.head())\n",
    "print(f\"\\nTotal de dias na série da estratégia: {df_ret_diario.shape[0]}\")\n",
    "\n",
    "# Salvar em Excel\n",
    "output_path = Path(\"retornos_diarios_estrategia_z10_comCDI_e_custo.xlsx\")\n",
    "df_ret_diario.to_excel(output_path, index=False)\n",
    "\n",
    "print(f\"\\nArquivo de retornos diários salvo em: {output_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "ee409d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Período filtrado:\n",
      "2017-01-02 00:00:00 → 2024-12-30 00:00:00\n",
      "Colunas de retornos log usadas:\n",
      "LONG: Ret_log_LONG\n",
      "SHORT: Ret_log_SHORT\n",
      "TOTAL_bruto: Ret_log_TOTAL_bruto\n",
      "TOTAL_liq: Ret_log_TOTAL_liq\n",
      "CDI: Ret_log_CDI_LogRet_D\n",
      "    Data_dia  Ret_log_LONG  LONG_cum_log  LONG_cum_simple  Ret_log_SHORT  \\\n",
      "0 2017-01-02      0.040557      0.040557         0.041391       0.008176   \n",
      "1 2017-01-03      0.028499      0.069056         0.071496      -0.036909   \n",
      "2 2017-01-04      0.014051      0.083107         0.086658      -0.011948   \n",
      "3 2017-01-05      0.023035      0.106143         0.111980      -0.017725   \n",
      "4 2017-01-06     -0.000006      0.106136         0.111973       0.009275   \n",
      "\n",
      "   SHORT_cum_log  SHORT_cum_simple  Ret_log_CDI_LogRet_D  CDI_cum_log  \\\n",
      "0       0.008176          0.008209              0.000507     0.000507   \n",
      "1      -0.028733         -0.028324              0.000507     0.001014   \n",
      "2      -0.040682         -0.039865              0.000507     0.001521   \n",
      "3      -0.058407         -0.056734              0.000507     0.002028   \n",
      "4      -0.049132         -0.047945              0.000507     0.002535   \n",
      "\n",
      "   CDI_cum_simple  Ret_log_TOTAL_liq  ESTRAT_LIQ_cum_log  \\\n",
      "0        0.000507           0.047939            0.047939   \n",
      "1        0.001015          -0.006843            0.041096   \n",
      "2        0.001522           0.002776            0.043872   \n",
      "3        0.002030           0.006221            0.050094   \n",
      "4        0.002538           0.009771            0.059865   \n",
      "\n",
      "   ESTRAT_LIQ_cum_simple  \n",
      "0               0.049107  \n",
      "1               0.041952  \n",
      "2               0.044849  \n",
      "3               0.051370  \n",
      "4               0.061693  \n",
      "\n",
      "Arquivo com retornos acumulados salvo em: C:\\Users\\Hugo Villanova\\Desktop\\DQAI2025\\Metodologia - V3\\retornos_acumulados_estrategia_z10_desde2017.xlsx\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ==========================================\n",
    "# 1) Carregar retornos diários da estratégia\n",
    "# ==========================================\n",
    "\n",
    "ret_diario_path = Path(\"retornos_diarios_estrategia_z10_comCDI_e_custo.xlsx\")\n",
    "df_ret = pd.read_excel(ret_diario_path)\n",
    "\n",
    "# Garantir tipo de data\n",
    "df_ret[\"Data_dia\"] = pd.to_datetime(df_ret[\"Data_dia\"])\n",
    "\n",
    "# Filtrar a partir de 2017-01-01\n",
    "df_ret = df_ret[df_ret[\"Data_dia\"] >= pd.Timestamp(\"2017-01-01\")].copy()\n",
    "df_ret = df_ret.sort_values(\"Data_dia\").reset_index(drop=True)\n",
    "\n",
    "print(\"Período filtrado:\")\n",
    "print(df_ret[\"Data_dia\"].min(), \"→\", df_ret[\"Data_dia\"].max())\n",
    "\n",
    "# ==========================================\n",
    "# 2) Identificar colunas de retorno log\n",
    "# ==========================================\n",
    "\n",
    "# Colunas que já sabemos\n",
    "col_long   = \"Ret_log_LONG\"\n",
    "col_short  = \"Ret_log_SHORT\"\n",
    "col_total_b = \"Ret_log_TOTAL_bruto\"\n",
    "col_total_l = \"Ret_log_TOTAL_liq\"\n",
    "\n",
    "# Encontrar coluna do CDI (é a outra que começa com Ret_log_ e não é as de cima)\n",
    "known_cols = {col_long, col_short, col_total_b, col_total_l}\n",
    "cdi_cols = [c for c in df_ret.columns if c.startswith(\"Ret_log_\") and c not in known_cols]\n",
    "assert len(cdi_cols) == 1, f\"Esperava encontrar 1 coluna de CDI em log, encontrei: {cdi_cols}\"\n",
    "col_cdi = cdi_cols[0]\n",
    "\n",
    "print(\"Colunas de retornos log usadas:\")\n",
    "print(\"LONG:\", col_long)\n",
    "print(\"SHORT:\", col_short)\n",
    "print(\"TOTAL_bruto:\", col_total_b)\n",
    "print(\"TOTAL_liq:\", col_total_l)\n",
    "print(\"CDI:\", col_cdi)\n",
    "\n",
    "# ==========================================\n",
    "# 3) Função auxiliar para acumular retornos\n",
    "# ==========================================\n",
    "\n",
    "def add_cumulative_columns(df: pd.DataFrame, col_log: str, prefix: str):\n",
    "    \"\"\"\n",
    "    A partir de uma coluna de log-retorno diário (col_log), adiciona:\n",
    "\n",
    "      - f\"{prefix}_cum_log\"      : soma cumulativa dos log-retornos\n",
    "      - f\"{prefix}_cum_simple\"   : retorno simples cumulativo = exp(cum_log) - 1\n",
    "    \"\"\"\n",
    "    cum_log_col = f\"{prefix}_cum_log\"\n",
    "    cum_simple_col = f\"{prefix}_cum_simple\"\n",
    "\n",
    "    df[cum_log_col] = df[col_log].cumsum()\n",
    "    df[cum_simple_col] = np.exp(df[cum_log_col]) - 1.0\n",
    "\n",
    "    return df\n",
    "\n",
    "# ==========================================\n",
    "# 4) Adicionar retornos acumulados para cada perna\n",
    "# ==========================================\n",
    "\n",
    "df_acum = df_ret.copy()\n",
    "\n",
    "# LONG\n",
    "df_acum = add_cumulative_columns(df_acum, col_long, \"LONG\")\n",
    "\n",
    "# SHORT\n",
    "df_acum = add_cumulative_columns(df_acum, col_short, \"SHORT\")\n",
    "\n",
    "# CDI\n",
    "df_acum = add_cumulative_columns(df_acum, col_cdi, \"CDI\")\n",
    "\n",
    "# Estratégia - vamos usar a perna TOTAL líquida (já com custo)\n",
    "df_acum = add_cumulative_columns(df_acum, col_total_l, \"ESTRAT_LIQ\")\n",
    "\n",
    "# (Opcional) se quiser também a versão bruta:\n",
    "# df_acum = add_cumulative_columns(df_acum, col_total_b, \"ESTRAT_BRUTA\")\n",
    "\n",
    "# ==========================================\n",
    "# 5) Ver uma amostra e salvar\n",
    "# ==========================================\n",
    "\n",
    "cols_view = [\n",
    "    \"Data_dia\",\n",
    "    col_long,\n",
    "    \"LONG_cum_log\", \"LONG_cum_simple\",\n",
    "    col_short,\n",
    "    \"SHORT_cum_log\", \"SHORT_cum_simple\",\n",
    "    col_cdi,\n",
    "    \"CDI_cum_log\", \"CDI_cum_simple\",\n",
    "    col_total_l,\n",
    "    \"ESTRAT_LIQ_cum_log\", \"ESTRAT_LIQ_cum_simple\",\n",
    "]\n",
    "\n",
    "print(df_acum[cols_view].head())\n",
    "\n",
    "output_path = Path(\"retornos_acumulados_estrategia_z10_desde2017.xlsx\")\n",
    "df_acum.to_excel(output_path, index=False)\n",
    "\n",
    "print(f\"\\nArquivo com retornos acumulados salvo em: {output_path.resolve()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
